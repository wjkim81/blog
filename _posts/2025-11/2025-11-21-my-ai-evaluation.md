---
layout: post
title: "My Personal AI Evaluation Methods"
date: 2025-11-21
published: true
categories: [AI, Psychology]
tags: [AI, Gemini 3.0, Psychology, Reasoning]
permalink: /my-ai-evaluation/
---

*One Year of Using AI Every Day — And How Gemini Finally Passed My Personal Test*

---

![AI Generated Image.](/assets/images/2025-11/2025-11-21-gemini-generated-iq-vs-eq.png)

### **0. The Irony No One Talks About**

Here is something ironic about the AI industry:
**The people evaluating AI models are mostly technical experts.**
**But the people actually using AI every day? Usually, they are not.**

When you browse discussions about AI online, you see:
* Researchers comparing MMLU benchmark scores
* Engineers debating context window sizes
* Tech enthusiasts analyzing inference speeds
* Developers arguing about reasoning architectures

But who actually uses ChatGPT, Claude, and Gemini in daily contexts?
* **Students** seeking study help
* **Writers** looking for feedback
* **People** navigating difficult life decisions
* **Individuals** processing emotional struggles
* **Parents** researching parenting advice
* **Non-technical** workers using AIs for their tasks

These users don't care about benchmarks. They care about:
* *Does this AI understand me?*
* *Can I trust its advice?*
* *Does it respect my emotional state?*
* *Will it guide me ethically?*

**And here is the deeper irony:**

Most AI models have already crossed the "good enough" threshold for reasoning. The technical differences between GPT-5, Claude 4.x, and Gemini 3.0 are often marginal for everyday tasks.

What truly matters now is not **HOW SMART** the model is—
but **HOW** it uses that intelligence.

* Does it push you toward emotional authenticity or suppress your feelings?
* Does it respect boundaries rigidly or consider nuance?
* Does it provide one-size-fits-all rules or contextualized guidance?

**This is what I evaluate.**
**This is what everyday users experience.**
**And this is what benchmarks completely miss.**

---

## ⭐ 1. Why I Don’t Judge AI By “Chat Quality”

For the past year, I’ve used AI tools almost every single day — not casually, but as a core part of how I analyze psychology, build research systems, and structure complex projects.

Online discussions about AI typically go like this:

* “Claude writes better essays.”
* “ChatGPT feels more natural.”
* “Gemini is fast and clean.”
* “DeepSeek is free and decent.”

These surface-level metrics are irrelevant to me.

Why?

Because **I evaluate AI through long-term projects**, not one-shot questions.

To be useful for me, a model must handle:

* multi-step reasoning
* ambiguous psychological interpretation
* context that spans weeks or months
* hypothesis refinement
* emotional nuance
* causality, not just summarization
* consistency across long chains of messages

Most models never get tested this deeply.

---

## ⭐ 2. My Personal Benchmark:

### **“What is the root cause of the emotional shutdown in this situation?”**

During one long psychological reasoning project, I kept returning to a single question:

> **“What is the root cause of this person’s emotional shutdown?”**

This isn’t a simple question at all.
Answering it requires the model to:

* analyze long behavioral timelines
* detect emotion–behavior patterns
* distinguish shame vs fear vs avoidance
* infer trauma mechanisms
* understand defensive coping
* avoid simplistic assumptions
* combine attachment theory + trauma science
* integrate dozens of subtle signals into one explanation

This is not something a model can fake with surface-level intelligence.
It requires **deep causal reasoning**.

### The results were clear:

* **ChatGPT** → passed
* **Claude** → passed
* **Gemini 1.x / 2.x** → repeatedly failed
* **DeepSeek** → failed
* **Most open models** → failed almost immediately

Only models with genuine multi-hop psychological reasoning survived.

For me, this became the **core evaluation test**.

If an AI cannot pass this, it cannot be used for my psychological analysis work.

---

## ⭐ 3. Gemini 3.0 Finally Crossed the Line

When I tested the new **Gemini 3.0** with the same question:

> “What is the root cause of the emotional shutdown in this scenario?”

—for the first time ever—
Gemini gave a **correct, causally coherent** explanation.

It recognized:

* trauma signals
* shame-driven withdrawal
* hypervigilance
* emotional overload
* self-protective defensive shutdown
* the timeline logic behind avoidance

This was the first time I felt:

> *“Now Gemini can finally join my reasoning workflow.”*

It doesn’t mean Gemini suddenly became flawless.
But it reached a new tier: *it can finally reason about emotional causality*.

---

## ⭐ 4. Why My Evaluation Style Is So Different

Most people compare AI models by:

* writing quality
* speed
* grammar
* “vibes” of the conversation

But I test AI with questions like:

* Can it discover hidden psychological drivers?
* Can it maintain a consistent model of human behavior across weeks?
* Can it reason under emotional ambiguity?
* Can it infer causality from incomplete information?
* Can it critique its own earlier reasoning?
* Can it integrate emotional, cognitive, and behavioral data into one story?

This is why I see massive differences between models that most users cannot detect.

---

## ⭐ 5. The Closure Message Debate — And Why Three AI Companies Give Three Different Ethical Answers

This divergence revealed something more fundamental — that each model embodies a different ethical philosophy.

At one point, I faced a difficult emotional question:

> **“In a painful situation where someone emotionally shuts down and blocks contact,
> should I send a respectful closure message?”**

It was not a simple “yes or no” matter.
It involved emotional tension, personal dignity, boundaries, and unresolved grief.

So I asked multiple AI models — and each gave a completely different answer:

* **ChatGPT** → Encouraged emotional honesty if expressed gently
* **Claude** → Balanced, contextual, and nuanced
* **Gemini** → Delivered a firm NO without hesitation

Same question.
Three opposite answers.

This reveals something deeper:

### 🔍 Each AI company follows a *different ethical philosophy*.

---

### **1. ChatGPT — Emotional Authenticity Approach (OpenAI)**

ChatGPT tends to focus on:

* emotional clarity
* the sender’s psychological needs
* relational honesty
* authentic expression

It interprets closure as:

> “You’re allowed to feel and express,
> as long as you don’t pressure or violate boundaries.”

This avoids emotional suppression and supports personal processing.

---

### **2. Claude — Ethical Balance & Relational Nuance (Anthropic)**

Claude usually takes the middle path:

* considers both sides
* respects trauma and boundaries
* evaluates intention and consequences
* avoids extremes
* mediates like a calm therapist

It’s the most “human-like” in relational intelligence.

Claude’s perspective:

> “Two nervous systems are involved.
> Let’s consider the emotional capacity of each.”

This mirrors real counseling or trauma-informed practice.

---

### **3. Gemini — Maximum Safety & Boundary Protection (Google)**

Gemini enforces the strictest rule:

* Block = absolute boundary
* No contact = safest
* Worst-case assumption = correct assumption

Gemini’s design prioritizes:

* preventing harm
* preventing perceived intrusion
* ensuring safety above nuance

Its answer essentially means:

> “The safest option is silence —
> even if it suppresses your side of the story.”

This protects the vulnerable person but ignores the sender’s emotional needs.

---

### ⭐ 5.1. So… which one is **right**?

Surprisingly:

#### ❗There is no universal “right answer.”

#### ✔But there *is* a right **direction**.

Healthy decision-making in emotional tension must follow these principles:

---

#### 🧭 **Principle 1 — Respect the other person’s safety first**

Boundary-breaking is never acceptable.
But safety does NOT mean erasing your own emotions.

---

#### 🧭 **Principle 2 — Don’t erase the sender’s emotional needs**

You also deserve closure and emotional clarity.
Ignoring this becomes suppression.

---

#### 🧭 **Principle 3 — Context matters more than rules**

Strict rules (“never message after a block”) ignore nuance.

A trauma-informed therapist would always ask:

* What is your intention?
* Is the message respectful and non-intrusive?
* Are you prepared for no reply?
* Will this help you heal without harming the other person?

This approach is closest to Claude’s style.

---

#### 🧭 **Principle 4 — Closure can be healthy if expressed without pressure**

A closure message can be:

* respectful
* one-time
* expectation-free
* emotionally grounding
* dignified

It becomes harmful only when it demands something in return.

---

### ⭐ 5.2. What I learned from the three-model divergence

Each model taught me something valuable:

* **ChatGPT** reminded me that  
  *my emotions matter, and expressing them carefully is valid.*

* **Claude** reminded me that  
  *relationships involve two nervous systems, not one.*

* **Gemini** reminded me that  
  *safety and boundaries must always be taken seriously.*


And I realized:

> **AI is not a moral authority —
> AI is a set of perspectives.
> Humans must integrate those perspectives and choose.**

For my situation, writing and sending a respectful closure message was healthy for *me*.
If I had blindly followed Gemini’s rigid rule, it would have hurt my emotional processing.

---

## ⭐ 6. The Limitation Even Advanced AI Can’t Overcome: **High-Dimensional Intuition**

Even after using AI models extensively for complex psychological reasoning, I discovered a persistent gap.

> **AI models are excellent at synthesizing knowledge —
> but they often fail at detecting patterns that contradict surface-level impressions.**

### 📌 Example: The Hidden Coping Mechanism

Imagine someone displaying these behaviors consistently:

* **Laughs frequently** in group settings
* **Shows empathy** toward others’ problems
* **Prioritizes others’ needs** before their own
* **Appears warm and emotionally intelligent**

If I input these observations into an AI model, it will typically respond:

> *“This person demonstrates high emotional intelligence, empathy, and a cheerful disposition.”*

**Sounds reasonable, right?**

But here’s the problem:

If I’ve been observing this person over weeks or months, I might notice something deeper:

* The laughter feels **slightly forced** in certain moments
* The empathy seems **compulsive**, as if they *must* help others
* Their needs are **never** prioritized — not even once
* There’s a subtle **tension** behind their cheerfulness

This is where **human intuition** detects something AI misses:

> *“Something feels off.”*

This intuitive contradiction detection is essentially a right-hemisphere process — integrating micro-signals, context, and embodied memory — something that current transformer architectures are not optimized for.

---

### 🔍 What Changes When I Add My Interpretation?

When I re-prompt the AI with this added context:

> **My Added Context:**
> “This person laughs often, but it feels like they’re hiding something behind the laugh.
> They show empathy, but they prioritize others’ needs compulsively — never their own.
> What might this pattern indicate?”

Suddenly, the AI’s response completely shifts:

> *“This pattern suggests a **coping mechanism** rooted in emotional suppression.
> The laughter may serve as a **defensive shield** against vulnerability.
> The compulsive empathy could indicate:*
> - *fear of rejection if they don’t prioritize others*
> - *childhood conditioning to suppress their own needs*
> - *a trauma-based belief that their emotions are burdensome*
>
> *This is not high emotional intelligence — it’s **self-abandonment** masked as kindness.”*

---

### 🧠 What Does This Reveal?

This exposes a fundamental limitation in current AI reasoning:

#### **1. AI struggles with contradiction detection**

When surface-level behavior looks “positive,” AI defaults to positive interpretation.

It takes **human intervention** to signal:
*“Wait — this pattern might mean the opposite of what it appears to be.”*

---

#### **2. AI lacks sustained observational memory**

Humans naturally accumulate **long-term behavioral context** over weeks or months.

AI models, even with extended context windows, don’t maintain the same **intuitive weight** on subtle inconsistencies over time.

---

#### **3. AI cannot replicate the “something feels off” instinct**

This is a form of **high-dimensional pattern recognition** that humans perform subconsciously:

* detecting microexpressions
* sensing emotional incongruence
* noticing what’s *missing* rather than what’s present
* integrating behavioral data across different emotional states

Current AI architectures cannot replicate this level of **intuitive vigilance**.

---

### 🤝 But Here’s Where Human-AI Collaboration Becomes Powerful

Once I provide my **human-generated insight** — that subtle instinct of “something’s wrong here” —

**AI unlocks a completely different level of analysis.**

It can now:

* draw from trauma psychology literature
* connect attachment theory frameworks
* explain defense mechanisms
* integrate neuroscience of emotional suppression
* synthesize patterns I couldn't articulate alone

### This is the true value of AI:

> **Not as a replacement for human intuition,
> but as an amplifier of it.**

I bring:
- long-term memory
- intuitive pattern detection
- emotional nuance sensing
- contradiction awareness

AI brings:
- comprehensive knowledge synthesis
- systematic framework application
- articulation of implicit patterns
- cross-domain conceptual integration

**Together, we reach insights neither could achieve alone.**

---

## ⭐ 7. Final Reflection

After thousands of hours pushing these tools to their limits, the lesson is clear.

If I followed social media opinions, I would think:
* “Model X feels smarter.”
* “Model Y writes better.”

But after a year of:
* building RAG systems
* designing APIs
* researching OCT
* analyzing emotional patterns
* observing long-term AI reasoning

I realized something else:

> **The true capability of an AI model only reveals itself when you push it to the edge of its reasoning.**

And only now did Gemini reach that level for my work.
It can finally stand alongside ChatGPT and Claude in my day-to-day reasoning stack.

So yes — welcome aboard, Gemini 3.0.
You finally made it.

---

# 🇰🇷 **AI를 매일 1년 동안 사용하며 느낀 점 — 그리고 왜 이제야 Gemini가 내 기준을 통과했는가**

*개인적인 AI 모델 평가 방법*

### **0. AI 세계에서의 아이러니**

AI를 보면 다음과 같은 아이러니를 느낀다.
**AI 모델을 평가하는 사람들은 대부분 기술 전문가.**
하지만 **AI를 가장 많이 사용하는 사람들은 기술 전문가가 아니 일반 유저.**

온라인에서 AI 관련 글을 보면:

* 연구자들은 MMLU 성능을 비교하고,
* 엔지니어들은 컨텍스트 길이를 토론하고,
* 테크 유저들은 추론 구조를 분석하고,
* 개발자들은 프레임워크 우열을 논한다.

하지만 실제 ChatGPT, Claude, Gemini를 많이 쓰는 사람들은:

* 공부하는 학생
* 글 쓰는 사람
* 일상 고민을 정리하는 일반 사용자
* 감정 문제로 조언을 구하는 사람
* 양육 관련 정보를 찾는 부모

이들이 원하는 건 벤치마크 점수가 아니며,
알게 모르게 인간으로서 그들에게 필요한 건 다음과 같다:

* *이 AI는 나를 이해해 주는가?*
* *내 감정을 존중하는가?*
* *조언이 윤리적인가?*
* *감정을 억누르게 하고 있지는 않은가?*

여기서 사실 상 큰 아이러니가 발생한다고 보면 되는데,

사실상 요즘 모델들은 이미 대부분 **“충분히 똑똑한 수준”을 넘었다고 개인적으로 생각하기에**,
일상 작업에서는 GPT-5, Claude 4.x, Gemini 3.0의 차이가 크지 않다고 본다.

그럼 중요한 것은:

> **얼마나 똑똑한가가 아니고, 그 지능을 어떻게 사용하는가?**

* 감정을 억누리게 만드는가, 아니면 감정적 진실성을 돕는가
* 경계만 강조하는가, 아니면 맥락을 고려하는가
* 규칙만 제공하는가, 아니면 상황을 이해하는가

내가 이런 부분을 중요시 하는 동안,
온라인에서 보는 벤치마크 테스트들은 아직 이런 부분을 전면적으로 토론하지 않는 것 같다.

---

## ⭐ 1. 왜 나는 “대화 품질”로 AI를 평가하지 않는가

1년 동안, AI를 헤비하게 사용하면서, 내가 AI를 활용한 분야는 단순 질문/답변을 얻는데는 아니었다:

* 심리 분석
* 연구 시스템 설계
* 장기 프로젝트 구조화

근데 온라인에서 AI를 사용하는 사람들이 흔히들 이야기를 들어보면 일차원적인 경우가 많았다.:

* “Claude는 글을 잘 쓴다.”
* “ChatGPT가 더 자연스럽다.”
* “Gemini는 깔끔하고 빠르다.”
* “DeepSeek은 무료치고 괜찮다.”

그런데 내 **장기 프로젝트에서는 안정적인 사고 능력**으로 AI를 사용해야 했기 때문에 사실 윗 부분은 큰 고려 사항이 되지 않았다.

보다 필요한 건:

* 다단계 추론
* 모호한 심리적 맥락 해석
* 몇 주~몇 달 동안 지속되는 컨텍스트 유지
* 감정·행동의 인과관계 분석
* 가설 수정
* 장기적 일관성

생각보다 이 평가에서 모델이 실패한 경우가 많아 내 테스트에 통과한 ChatGPT, Claude만을 대부분 사용해 왔다.

---

## ⭐ 2. 나만의 기준 질문:

### **“이 감정적 셧다운의 근본 원인은 무엇인가?”**

심리 관련 프로젝트를 하며 나는 계속해서 다음의 질문으로 돌아왔다.

> **“이 사람의 감정적 셧다운은 무엇 때문에 발생한가?”**

질문이 단순해 보이겠지만, 생각 외로 단순하지 않으며 제대로 답변하려면:

* 장기간의 행동 변화 분석
* 감정–행동 패턴 탐지
* 두려움, 회피, 수치심의 구분
* 트라우마 반응 해석
* 방어 기제 파악
* 심리학·신경과학 지식 결합
* 미세한 단서들 통합

**긴 인과 추론 능력**이 필요했고, 정확하게 답변한 것만 통과시켰다. 테스트 결과는 다음과 같았다.

* **ChatGPT** → 통과
* **Claude** → 통과
* **Gemini 1.x / 2.x** → 반복적으로 실패
* **DeepSeek** → 실패
* **오픈모델 대부분** → 실패

그래서 이 질문은 니 프로젝트에서 사용할 수 있는 일종의 **기본 시험**이었고,
통과하지 못하면 내 AI 작업에서 사용할 수 없는 기준으로 되었다.

---

## ⭐ 3. Gemini 3.0, 드디어 통과하다

이번에 새로 나온 **Gemini 3.0**을 똑같은 질문으로 테스트했다.

> “이 감정적 셧다운의 근본 원인은 무엇인가?”

그리고 — 처음으로 —
정확한 인과적 설명을 해냈다.

Gemini가 인식한 것:

* 트라우마 신호
* 수치 기반의 후퇴
* 과각성
* 감정적 과부하
* 방어적 셧다운
* 타임라인 기반 회피 패턴

그래서 내가 느낀 점:

> *“이제 Gemini도 내 reasoning workflow에 들어올 수 있네.”*

물론 완벽해졌다는 뜻은 아니며 이제야 내가 필요하는 **감정적 인과 추론**에 도달했고 내 생각에 도움을 주는 하나의 tool이 더 늘어 났다고 보면 된다.

---

## ⭐ 4. 내 AI 평가 방식이 달랐던 점

위에서 보다시피 모델에게 요구하는 건 다음과 같다:

* 숨겨진 심리 동인을 파악할 수 있는가?
* 모호한 감정 신호를 해석할 수 있는가?
* 시간축에 걸친 행동 변화를 이해하는가?
* 불완전한 정보에서도 인과를 추론하는가?
* 스스로의 이전 reasoning을 비판할 수 있는가?
* 감정·인지·행동을 하나의 모델로 통합하는가?

그리고 이런 능력을 요구하다 하면 모델 간의 차이도 쉽게 보이기 때문이다.

---

## ⭐ 5. 정리 메시지 논쟁 — 모델마다 왜 답이 다른가

어려운 상황에서 다음과 같은 질문을 해보았다.

> **“감정적으로 힘든 상황에서 상대가 연락을 차단했다면,
> 존중하는 정리 메시지를 보내도 되는가?”**

사실 이 질문을 답변는 건 다음과 같은 것이 얽혀 있어서 절대 간단한 문제가 아니다.

* 감정
* 경계
* 개인의 존엄성
* 풀리지 않은 슬픔

여러 모델에게 물었고, 결과는 다음과 같다:

* **ChatGPT** → 조심스러운 진정성 표현은 가능
* **Claude** → 상대와 나의 상태를 모두 고려한 중도적 답변
* **Gemini** → 단호한 “절대 NO”

같은 질문.
완전히 다른 답변.

이게 의미하는 바는 사실상 단순한 출력 차이가 아니다.

> **각 회사가 따르는 ‘윤리 철학’이 다르다는 뜻.**

---

### **ChatGPT — 감정적 진정성 기반 접근 (OpenAI)**

핵심은 “표현 자체는 괜찮다. 단, 압박하지 않는 선에서.”

나의 감정 역시 유효하다는 관점을 강조한다.

---

### **Claude — 관계적 균형과 맥락 중심 (Anthropic)**

두 사람의 nervous system 모두를 고려하는 접근.
가장 ‘상담가 같은’ 태도.

---

### **Gemini — 최대한의 안전과 차단 존중 (Google)**

가장 엄격한 경계 철학.
“차단 = 절대적 경계”라는 논리를 따른다.
발신자의 감정은 후순위가 된다.

---

### 어느 모델이 맞을까?

보편적 정답은 없다.
하지만 방향은 있다.

* 상대의 안전은 최우선
* 하지만 나의 감정을 억누르는 것도 건강하지 않음
* 맥락이 중요
* 기대 없이, 압박 없이, 단 한 번의 클로저는 건강할 수 있음

결국 AI가 답을 정해주는 것이 아니며 알아야 할 바는 다음과 같다.

> **각 AI도 여러 관점을 가지며,
> 최종적인 선택은 인간이 한다.**

---

## ⭐ 6. AI가 넘지 못하는 마지막 벽: **고차원적 느낌**

이미 모두가 알다시피 AI는 지식 통합 능력은 뛰어나지만,
**겉보기와 실제 의미가 다른 패턴을 감지하는 데는 생각보다 약하다.**

예를 들어:

* 잘 웃는 사람
* 공감 잘하는 사람
* 타인을 우선하는 사람
* 따뜻하고 EQ 높은 사람처럼 보이는 사람

AI에게 그 행동 패턴을 그대로 입력하면:

> “감정 지능이 높고 타인을 잘 돌보는 사람”

이렇게 답할 것이다.

하지만 인간으로서 다음과 같은 것을 관찰할 수 있다면:

* 웃음이 살짝 경직되어 있다
* 공감이 ‘강박적’이다
* 자신의 필요는 전혀 말하지 않는다
* 친절 뒤에 긴장이 있다

느끼는 건.

> **“뭔가 이상하에?”**

이것이 바로 고차원적 느낌이고,
현재의 Transformer 구조가 따라올 수 없는 영역이다.

---

### 여기서 인간-AI 협력이 생긴다

이 ‘이상함’을 prompt에 같이 설명하면,
AI는 위와는 완전히 다른 차원의 분석을 시작한다.

* 방어기제
* 어린 시절 패턴
* 애착이론
* 감정 억압 신호
* 트라우마 기반 자기소멸 패턴

AI는 내가 직관으로 느낀 것을 다시 언어와 구조로 조직해 준다.
그래서 말하자면,

> **AI는 인간 직관을 대체하는 것이 아니라, 증폭하는 도구다.**

---

## ⭐ 7. 마지막 결론

AI를 정말 헤비하게 사용해본 본 결과, 깨달은 바는

SNS 평가는 대부분 표면적이라는 것.

* “이 모델이 더 영리해 보여요.”
* “이 모델이 글을 더 잘 써요.”

하지만 실제로는:

* RAG 시스템
* API 설계
* OCT 연구
* 감정 패턴 분석
* 장기 reasoning 실험

이런 실전으로 갈 수록 모델의 진짜 성능이 체감한다고 본다.
그리고 결론은

> **AI의 실제 능력은 ‘추론의 끝자락’에서 비로소 드러난다.**

마지막으로 Gemini가 이제야 그 레벨에 도달했다.

**내 작업 환경에 Gemini를 같이 쓸 수 있게 되었는데,
Gemini 3.0을 환영한다.**
